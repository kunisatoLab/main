---
title: "認知課題の解析データセット作成"
---
## 認知課題データの解析データセット作成法
### 統計解析よりも解析データセット作成が大切！

解析の再生可能性を高めるうえで，統計解析自体よりも解析データセット作成が大切です。特に認知課題から得られるデータは，統計解析にかけるまでのデータセットの作成段階で色々と操作が必要です（不要なものを除外したり，複数の参加者のデータを統合したり，変数に処理を加えたりなど）。その際に，ミスが生じがちです。これまで学生指導していて，データ解析でミスが混入する危険性が高いのがここです。この段階でのミスは発覚しにくく，後で追えないこともあり，最もデータ処理上気をつけなくてはいけない部分です。以下では，これらの問題をクリアするために，Rを用いた解析データセットの作成方法について解説します。

<br />

### 解析データセット作成ではExcelを使わない

最初に，解析データセットの作成では，**Excelの使用を禁止**します。ExcelもSPSSもマウスを使って操作をしますが，そのような操作はミスが生じやすく，その操作過程の記録が残らないことも多いので，注意が必要です（操作過程が残っていても追いにくい面もあります）。例えば以下のようなことがあります。

- 例1) 各参加者のデータを解析用エクセルファイルにコピー＆ペーストしていたが，参加者Aのデータをコピー＆ペーストしてから，参加者Bのデータをコピー＆ペーストしたが，実際にはコピーができておらず，参加者Aのデータが２回入ってしまった。

こういうことがあると，解析で得られた結果は真実とは違ったものになります。100%ミスせずにExcelでコピー＆ペーストできる保証はありません。

- 例2) Excel上でデータの確認のために，特定の変数の値で並び替えたが，その並び替えが特定の変数だけに指定されており，参加者の本当の値とは違う値が入ることになった。

こういうことがあると，解析で得られた結果は真実とは違ったものになります。ソートに限らず，なんらかの処理をエクセル上でしてしまうと，それが本当にただしい処理をしたのか後で確認できなくなることがあります。

### 解析データセット作成ではRを使おう

認知課題解析用データセットの作成は，以下の原則を守りつつ行う必要があります。

- オリジナルの生データを直接操作しない（必ず別途バックアップとっておく）
- オリジナルの生データから必要な情報をマウス操作ではない形で抽出する
- オリジナルの生データを１つのファイルにまとめて，解析可能なフォーマットに変更する
- 上記の操作について作業ログが残り，何度でもやり直せる

このような条件を満たすもので，心理学科の学生にも使いやすいものとしては**R**があります。解析データセット作成では，**R**を使いましょう！以下では，(1)jsPsychを用いたWEB実験で得られたJSONファイルの処理，(2)PsychoPyを用いた実験で得られたcsvファイルの処理について解説します。なお，RやRstudioの基本的な使い方については，[R言語の基本と解析梱包](how-to-R-language.html)で学んでいることを前提とします。


## jsPsychを用いたWEB実験で得られたJSONファイルの処理
### データとフォルダの準備

[jsPsychを用いたWEB実験のサンプルデータ](materials/stroop.zip)をクリックして，ダウンロードして，自分のパソコンの好きな場所に解凍・展開してください。解凍・展開すると，stroop.jsonというファイルが出てくると思います。


## Rstudioの作業ディレクトリーの設定
　
なお，パソコンで解析などの作業をする場合に，パソコンに，パソコン上のどこで作業をするのか，その場所を知らせる必要があります。そのような作業場所を作業ディレクトリーと言います。Rstudio上で，作業ディレクトリーを先程のAnalysisフォルダに設定します。具体的な操作は，　RStudioのメニューバーから、「Session」->「Set Working Directory」->「Choose Directory」で，先程のAnalysisフォルダを選択してください。

### 使用するRパッケージ
- ダウンロードしたJSONファイルを読み込む際には，以下のようにして，jsonliteパッケージを使います。

```{r}
library(jsonlite)
library(tidyverse)
db_list <- fromJSON("materials/stroop.json")
```

- glimpse()で，読み込んだJSONファイルの構造をチェックします。このデータは，私が３回Stroop課題に取り組んだものです。listは参加者で構成されていますので，３つあります。そして，その下に実験データのdata.frameがあります。

```{r}
glimpse(db_list)
```

- 読み込んだJSONデータをas.tibbleを使ってデータフレームにする。すると，３つの参加者が変数になって，変数の下にdata.frameができました。

```{r}
db_df <- as_tibble(db_list, validate = F) 
db_df
```

- これは少し扱いにくいので，getherでlongデータ化します。

```{r}
db_df2 <- gather(db_df, ID, data)
db_df2
```

- 各参加者にdata.frameがネストしちゃっているので，unnestします。これで，とりあえず解析データセットが完成しました。

```{r}
db_df3 <-unnest(db_df2)
db_df3
```

- 上記の操作＋整形作業すると，以下のコードになります。

```{r}
db_df <- db_list %>% 
  as_tibble(validate = F) %>%  #データフレーム化
  gather(ID, data) %>% 　　　　#ロング化
  unnest() %>% 　　　　　　　　#ネストやめる
  select(ID,trial_index,stim_type,response,correct,rt) %>%　#使う変数選択 
  filter(trial_index !=0) %>%　#ウェルカムメッセージは不要 
  filter(trial_index !=1) %>%  #教示は不要
  filter(trial_index !=19)     #最後の教示も不要

db_df
```






## PsychoPyを用いた実験で得られたcsvファイルの処理

## データとフォルダの準備

[PsychoPyを用いた実験のサンプルデータ](materials/Analysis.zip)をクリックして，ダウンロードして，自分のパソコンの好きな場所に解凍・展開してください。"Analysis"フォルダ内に"Data"フォルダがあり，その中に，sub01.csvからsub04.csvという名前のcsvファイルが入っているかと思います。sub01.csvからsub04.csvは，逆転学習課題についての４名分の仮想データになります。

今回は，すでにフォルダに名前がついていましたが，自分でフォルダ名を付ける場合は，日本語を使用するのは推奨しません。フォルダとファイルと変数名には，英語（日本語のローマ字化でけっこう）を使用してください。

## Rstudioの作業ディレクトリーの設定
　
なお，パソコンで解析などの作業をする場合に，パソコンに，パソコン上のどこで作業をするのか，その場所を知らせる必要があります。そのような作業場所を作業ディレクトリーと言います。Rstudio上で，作業ディレクトリーを先程のAnalysisフォルダに設定します。具体的な操作は，　RStudioのメニューバーから、「Session」->「Set Working Directory」->「Choose Directory」で，先程のAnalysisフォルダを選択してください。

## カレントディレクトリーの移動

基本的には，Analysisフォルダで解析の前処理や解析は行いますが，一時的にフォルダ移動する必要があります。その時に便利なのが，getwd()とsetwd()です。wdは，作業ディレクトリ(working directory)を意味し，getwdで現在の作業ディレクトリの情報を取得し，setwdで作業ディレクトリを設定します。

以下をRstudioのConsoleにタイプしてみましょう！

```
getwd()
```

おそらく，さきほど設定したAnalysisフォルダまでのパスが出力されたかと思います（そうじゃない人は，"Rstudioの作業ディレクトリーの設定"に戻って下さい）。


次に，Analysisフォルダの１つ下の階層にあるDataフォルダに移動してみましょう。以下をRstudioのConsoleにタイプしてみましょう！

```
workDir <- getwd()
setwd(paste(workDir, "Data", sep = "/"))
```

上記のコードでは，まず１行目で，getwd()で取得した作業ディレクトリのパスをworkDirに入れます。ここで，<-という矢印は，右側のものを左側のものにいれる（代入する）ということを意味します。次に，setwdを使って，workDir（作業ディレクトリのパス）とDataをpasteで結合し（sep="/"で/で区切るように設定），Analysis下のDataフォルダに作業ディレクトリを変更しています。上記を打ち込んだら，getwd()をタイプして，ちゃんとDataフォルダが作業ディレクトリになっているか確認しましょう！

なお，DataフォルダからAnalysisフォルダに戻るには，以下のsetwd("..")が便利です（".."で１つ上の階層に移動します）。

```
setwd("..")
```

上記のsetwd("..")をRstudioのConsoleに打ちこんでから，getwd()でAnalysisフォルダに戻ってきているか確認をしてみましょう。

## Dataフォルダ内のファイル名を取得

もう一度，Dataフォルダに移動します。その上で，list.filse()を使って，フォルダ内のファイル名をリスト化します（そしてfileNamesに入れる）。

```{r}
workDir <- getwd()
```

```{r, include=FALSE}
workDir <- paste(workDir, "materials/Analysis", sep = "/")
```


```{r}
setwd(paste(workDir, "Data", sep = "/"))
fileNames <- list.files()
```

Analysisフォルダに".."で戻ります。

```
setwd("..")
```

fileNamseの中身を確認します。フォルダに入っているファイル名がfileNamesに格納されているかと思います。

```{r}
print(fileNames)
```

## Dataフォルダ内のファイル数を確認

fileNamesに格納されているファイルの数から参加者数を確認します。lengthでデータの長さ（ここでは，fileNamesに含まれるデータの個数）がわかります。今回は，４個のデータが入っています。

```{r}
numberSubject <- length(fileNames)
print(numberSubject)
```

## sub01のデータを読み込んでみる

では，早速，sub01.csvを読み込んでみましょう！tidyverseに入っているreadrパッケージのread_csv()でcsvファイルが読み込めます。そして，読み込むファイル名は，fileNamesに格納されているのものの１つ目（つまり，sub01）です。

```{r, message=FALSE}
sub01 <- read_csv(paste(workDir, "Data",fileNames[1], sep = "/"))
sub01
```


## sub01のデータで必要な部分を抽出

上記のsub01のデータの解析で必要なのは，rt（反応時間）, key_press（押したキー），trial_index(試行番号)，correct（正誤）になります。また，trial_typeでは，categorizeのデータだけが欲しい（textは教示なので，いらない）。このようなデータの整理では，tidyverseのdplyrパッケージを使います。ここで，%>%というパイプ演算子というものがでてきます。これは，%>%の左(or前)のものが，次にくる関数の第一引数に入ることを意味します（例えば，上のlength(fileNames)の場合，length()関数の第一引数は，ファイル名リストのfileNamesになります）。


具体的な操作は以下になります。(1)まず，filterで，trial_typeが"categorize"なものにしぼります。(2)key_pressは，90だと紫の選択，77だと緑の選択になります。90と77だと扱いにくいので，90は1，77は0に変換します。ifelse(key_press == 90, 1,0)を使って，90なら1，それ以外は0になるようにして，mutateの新しい変数rewardを作成します（mutateは，新しい変数を作成する関数です）。(3)correctは，true,falseになるのですが，これも扱いにくいので，trueは1,falseは0にします。as.numeric(as.logical(correct))を使って，論理値にした上で，数値型に変換します。これもmutateを使って新たにrewardという名前を付けます。(4)この段階で，key_press, trial_type, time_elapsed, internal_node_id, correct, stimulusが要らなくなりました。selectを使って，不要な変数にマイナスをつけて除外します。(5)最後に，renameを使ってtrial_indexをnoに変更します。

```{r}
# データの整理
sub01 <- sub01 %>% 
  filter(trial_type=="categorize") %>% 
  mutate(choice = ifelse(key_press == 90, 1,0)) %>%
  mutate(reward = as.numeric(as.logical(correct))) %>% 
  select(-key_press, -trial_type, -time_elapsed, -internal_node_id, -correct, -stimulus) %>% 
  rename(no = trial_index) 
# データの確認
sub01
```

これで，必要な部分だけきれいに抽出できました。

## ４名のデータの読み込みと保存
### データの読み込み

さて，今度は４名分のデータ（フォルダ内のすべてのデータ）を読み込んで，整理してみましょう。フォルダ内のデータ数分（今回は４）だけ，上記の操作を繰り返します。繰り返す場合は，for文を使います。for文は以下のように書きます。1から"繰り返す回数"までを順番にiに代入しつつ，"繰り返したい操作"を繰り返します。

```
for(i in 1:繰り返す回数){
    繰り返したい操作
}
```

csvファイルを繰り返し読み込むだけでは，それぞれをバラバラに読み込むだけになります。そこで，analysisDataという変数を作って，新たにデータを読み込んで・整理したら，rbind()を使って結合するとう作業をしてみましょう。

```{r, message=FALSE}
# analysisDataの準備
analysisData <- NULL
# 1からnumberSubject分（4回），操作を繰り返す
for(i in 1:numberSubject){
  # 読み込んだcsvファイルのデータをtempDataに保存（tempは一時的を意味するtemporaryの省略です）。
  tempData <- read_csv(paste(workDir, "Data",fileNames[i], sep = "/"))
  # 上記とほぼ同じ操作をする（最後にデータ数分id番号を追加しています）
  tempData <- tempData %>%
    filter(trial_type=="categorize") %>% 
    mutate(choice = ifelse(key_press == 90, 1,0)) %>%
    mutate(reward = as.numeric(as.logical(correct))) %>% 
    select(-key_press, -trial_type, -time_elapsed, -internal_node_id, -correct, -stimulus) %>% 
    rename(no = trial_index) %>% 
    mutate(id = rep(i,length(rt)))
  #データの結合
  analysisData = rbind(analysisData, tempData)
}
# データの確認
analysisData
```


### データの保存

これで，４名分の生データを読み込んだので，今度は，これをcsvファイルとして保存します。csvファイルは，write.csv()を使って，保存できます。

```
write.csv(analysisData, "analysisData.csv")
```

### IDの対応表

上記の作業では，for文でのiをIDとしているが，それが実際のどのファイルに対応するのか対応づけできてない。そこで，以下では，上記と同じようなfor文を使って，iとフォルダのファイル名との対応づけをした対応表(idTable)を作成する。また，tidyverseのstringrパッケージを用いると（stringrパッケージはtidyverseを読み込むだけでは読み込まれないので，別途libraryで読み込む），文字列の一部を抽出することもできる。今回は，参加者ID番号にかかわる4~5番目の文字を抽出してみて，それをidTableにいれる。

```{r}
library(stringr)
idTable <- NULL
for(i in 1:numberSubject){
  idTable$csvName[i] <- fileNames[i]
  # csvのファイル名の4~5文字目を抽出（つまりsubと.csvの間のID番号の部分）
  idTable$csvId[i] <- str_sub(fileNames[i],4,5)
  idTable$dataId[i] <- i
}
idTable <- as.data.frame(idTable)
print(idTable)
```

## もう少し処理をしたデータの読み込みと保存
### データの読み込み

上記では，フォルダ内のすべてのcsvファイルを読み込んで，その生データを結合して，１つのcsvファイルで保存しました。私個人としては，このように各試行ごとのデータを使って解析した方が良いと考えますが，目的によっては，各試行ごとのデータではなく，全試行における正答率などを参加者ごとに計算した場合もあるかと思います。今回は，各参加者のデータを読み込んで，紫を選んだ比率（1が紫，緑は0）と正答率(correctでは1が正答，0が誤答)を各参加者ごとにsummaryDataに保存してみます。先程ののanalysiDataを作成したコードにちょっとだけ追加をしてみます。

```{r, message=FALSE}
# analysisDataの準備
analysisData <- NULL
# summaryDataの準備
summaryData <- NULL
# 1からnumberSubject分（4回），操作を繰り返す
for(i in 1:numberSubject){
  # 読み込んだcsvファイルのデータをtempDataに保存（tempは一時的を意味するtemporaryの省略です）。
  tempData <- read_csv(paste(workDir, "Data",fileNames[i], sep = "/"))
  # 上記とほぼ同じ操作をする（最後にデータ数分id番号を追加しています）
  tempData <- tempData %>%
    filter(trial_type=="categorize") %>% 
    mutate(choice = ifelse(key_press == 90, 1,0)) %>%
    mutate(reward = as.numeric(as.logical(correct))) %>% 
    select(-key_press, -trial_type, -time_elapsed, -internal_node_id, -correct, -stimulus) %>% 
    rename(no = trial_index) %>% 
    mutate(id = rep(i,length(rt)))
  #データの結合
  analysisData = rbind(analysisData, tempData)
  
  # ここまで一緒。ここから，summaryDataの追加作業
  summaryData$id[i] <- i
  # 紫の選択率
  summaryData$purpleRate[i] <- sum(tempData$reward)/length(tempData$reward)
  # 正答率
  summaryData$correctRate[i] <- sum(tempData$choice)/length(tempData$choice)
}
#データフレーム化と結果の表示
summaryData <- as_data_frame(summaryData)
summaryData
```

### データの保存

summaryDataをcsv形式で保存する。

```
write.csv(summaryData, "summaryData.csv")
```

## おわりに

これで，解析用のデータセットを作ることができました。あとは，Rで解析してもよし，SPSSやHADで解析しても良いです（個人的にはRを薦めます・・・）。Rの入門資料については，以下にスライドをアップしていますので，読んでみてください。

<iframe src="//www.slideshare.net/slideshow/embed_code/key/bdxbQ58hm1Bu5O" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/YoshihikoKunisato/r-74207682" title="臨床心理学におけるR入門: データハンドリングから統計解析まで" target="_blank">臨床心理学におけるR入門: データハンドリングから統計解析まで</a> </strong> from <strong><a target="_blank" href="//www.slideshare.net/YoshihikoKunisato">Senshu University</a></strong> </div>

また，研究室においてあるRの書籍や以下のサイトを参考にしつつRでデータハンドリング・視覚化・統計解析（モデリング）をしてみましょう！

- [「R for Data Science: Import, Tidy, Transform, Visualize, and Model Data」](http://amzn.asia/1BxQP81)（研究室の本棚においてあります）
- [Rで表の作成(Mr.Unadon氏のブログ)](https://mrunadon.github.io/TableWithR/)
- [Rで色々な図の作成(Mr.Unadon氏のブログ)]((https://mrunadon.github.io/ggplot2/))
- [Rで論文用の図の作成（Mr.Unadon氏のブログ）](https://mrunadon.github.io/ThesisPlot/)
- [Rチュートリアルセミナー特別テキスト（山口大学小杉先生・押江先生のPDF資料）](http://kosugitti.sakura.ne.jp/wp/wp-content/uploads/2014/01/R_tutorial2013.pdf)
