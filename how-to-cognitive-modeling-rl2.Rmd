---
title: "強化学習モデル: ベイズ推定(1)"
---

## 認知モデリングの推奨実践法

Busemeyer & Diederich(2010), Heathcote (2014), Palminteri et al.(2017)を参考にまとめると，以下のような感じになります。

- A 認知課題と認知モデルを準備
- B 人工データ生成とパラメータリカバリーを確認（モデルや課題の修正）
- C データ収集と行動データを確認
- D パラメータ推定
- E 相対モデル比較
- F モデル・シミュレーションと選択的影響テスト

<a href="https://kunisatolab.github.io/main/how-to-cognitive-modeling-rl1.html" target="_blank">「強化学習モデル: 最尤推定」</a>ではABCDとすすめてきました。今回は，ベイズ推定をcmdstanを使って行ってみます。そこで，一度Bに戻ってからDに取り組みます。

### 使用するRパッケージ

適宜必要なパッケージをインストールしてください。cmdstnrのインストールは，<a href="https://mc-stan.org/cmdstanr/" target="_blank">こちら</a>をご確認ください。

```{r message=FALSE, warning=FALSE}
rm(list = ls())
library(tidyverse)
library(cmdstanr)
library(rstan)
library(posterior)
library(bridgesampling)
```


## B 人工データ生成とパラメータリカバリーを確認（モデルや課題の修正）

### 人工データの生成

改めて人工データを生成するために，実験状況の入ったsim_dataの準備とq_learning_sim関数とq_learning_ll関数の準備をします。

```{r, echo=TRUE, message=TRUE, warning=TRUE}
sim_data <- tibble(trial = 1:80,
                   prob_s1 = rep(c(0.2, 0.8), each = 40),
                   prob_s2 = rep(c(0.8, 0.2), each = 40),
                   reward_s1 = ifelse(runif(80) < prob_s1, 1, 0),
                   reward_s2 = ifelse(runif(80) < prob_s2, 1, 0))

q_learning_sim <- function(alpha, beta,data) {
  #変数の準備
  value_s1 <- 0          # s1の価値(初期値は0)
  value_s2 <- 0          # s2の価値(初期値は0)
  current_choice <- NULL     # ある時点の選択（1=s1，0=s2）
  choice_prob_s1 <- NULL  # s1の選択確率
  reward <- NULL                 # 報酬
  # Qlearningモデル
  for (i in 1:nrow(data)){
    # s1を選ぶ確率を計算し,一様分布から発生させた乱数が行動選択確率よりも小さい時に1（s1），大きい時に0（s2）
    choice_prob_s1[i] <- exp(beta*value_s1[i])/(exp(beta*value_s1[i])+exp(beta*value_s2[i]))
    current_choice[i] <- as.integer(runif(1,min=0,max=1) <= choice_prob_s1[i])
    #FBを報酬(r)として、価値の更新を行う。
    if (current_choice[i] == 1){
        reward[i] <- data$reward_s1[i]
        #予測誤差の計算
        prediction_error <-  reward[i] - value_s1[i]
        #予測誤差を使ってs1の価値を更新する
        value_s1[i+1] <- value_s1[i]+alpha*prediction_error
        #s2は更新なし
        value_s2[i+1] <- value_s2[i]
    }else{
        reward[i] <- data$reward_s2[i]
        #予測誤差の計算
        prediction_error <- reward[i] - value_s2[i]
        #予測誤差を使ってs2の価値を更新する
        value_s2[i+1] <- value_s2[i]+alpha*prediction_error
        #s1は更新なし
        value_s1[i+1] <- value_s1[i]
    }
  }
  result <- data.frame(trial = data$trial,
              value_s1 = value_s1[1:nrow(data)], 
              value_s2 = value_s2[1:nrow(data)], 
              prob_s1 = choice_prob_s1,
              choice = current_choice,
              reward = reward)
  return(result)
}

q_learning_ll <- function(alpha, beta,data) {
  #変数の準備
  value_s1 <- 0          # s1の価値(初期値は0)
  value_s2 <- 0          # s2の価値(初期値は0)
  prob_s1 <- NULL  # s1の選択確率
  ll <- 0                 # 対数尤度
  # Qlearningモデル
  for (i in 1:nrow(data)){
    # s1を選ぶ確率を計算
    prob_s1[i] <- exp(beta*value_s1[i])/(exp(beta*value_s1[i])+exp(beta*value_s2[i]))
    #FBを報酬(r)として、価値の更新を行う。
    if (data$choice[i] == 1){
        #予測誤差の計算
        prediction_error <-  data$reward[i] - value_s1[i]
        #予測誤差を使ってs1の価値を更新する
        value_s1[i+1] <- value_s1[i]+alpha*prediction_error
        #s2は更新なし
        value_s2[i+1] <- value_s2[i]
        # 対数尤度の計算のために選択したs1を選ぶ確率の対数を加算
        ll <- ll + log(prob_s1[i])
    }else{
        #予測誤差の計算
        prediction_error <- data$reward[i] - value_s2[i]
        #予測誤差を使ってs2の価値を更新する
        value_s2[i+1] <- value_s2[i]+alpha*prediction_error
        #s1は更新なし
        value_s1[i+1] <- value_s1[i]
        # 対数尤度の計算のために選択したs2を選ぶ確率の対数を加算
        ll <- ll + log(1-prob_s1[i])
    }
  }
  result <- data.frame(trial = data$trial,
              value_s1 = value_s1[1:nrow(data)], 
              value_s2 = value_s2[1:nrow(data)], 
              prob_s1 = prob_s1,
              choice = data$choice,
              reward = data$reward)
  return(list(result = result, ll = ll))
}
```

alpha = 0.3, beta = 2で，シミュレーション・データを生成します。

```{r}
set.seed(1234)
data_1 <- q_learning_sim(alpha = 0.3, beta = 2, sim_data)
```

### cmdstanrでの推定
#### Stanコードの作成

"q_learning.stan"というファイルを作成して，以下のコードを書き込みます。Stanは統計モデリング用のプラットフォームで，MCMCサンプリングによるベイズ推定，変分推定，最適化による最尤推定が可能です。Stanでは，data,parameters,modelのようにブロックごとに指定をして，書いていきます（この３つが最小限のブロック数かと思います）。dataブロックでは入力するデータについて記述します（型と範囲の指定が必要です）。parameterブロックでは，推定するパラメータについて記述します（型と範囲の指定が必要です）。modelブロックでは，データとパラメータを用いた生成モデルの記載をします。なお，データ~分布(y ~ normal(mu, sig))のような形で記述もできますし，target += 対数尤度のように，対数尤度でも記述ができます。以下は，Q学習の推定用のコードですが，グリッドサーチで用意したものに比べて，すっきりしているかと思います。

```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}
parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
  real<lower=0.0> beta;            //逆温度
}
model {
  //学習率と逆温度の事前分布の指定はしていないので，parametersで指定した範囲の無情報事前分布が使われる
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}
```

Stanコードがstanファイルとして保存できたら，cmdstan_model()コンパイルします。Rは便利ですが計算は遅い言語なので，c++のような高速な計算が可能な言語にコンパイルをします。ちなみに，stanをRで使う場合は，Rstanを使うことが多かったのですが，最近は，cmdstanrが開発されており，こっちのほうがコンパイルもサンプリングも早いのでおすすめです。

```{r include=FALSE}
q_learning <- cmdstan_model('rl/q_learning.stan')
```

```
q_learning <- cmdstan_model('q_learning.stan')
```

### 最尤推定

コンパイルができたら，最適化による最尤推定をしてみましょう。最尤推定は，<a href="https://kunisatolab.github.io/main/how-to-cognitive-modeling-rl1.html" target="_blank">「強化学習モデル: 最尤推定」
</a>でも見てきたように，optimやpsoなどのRパッケージでできますが，ここではstanで最尤推定値を推定します。コンパイルしたモデル$optimize()で推定ができます。

```
mle_cmdstan <- q_learning$optimize(
  data = list(trial = nrow(data_1),
              reward = data_1$reward,
              choice = data_1$choice + 1),
  seed = 123)
mle_cmdstan$summary()
```


```{r include=FALSE}
q_learning_mle <- q_learning$optimize(
  data = list(trial = nrow(data_1),
              reward = data_1$reward,
              choice = data_1$choice + 1),
  seed = 123)
q_learning_mle$summary()
```


### パラメータリカバリ（最尤推定）

stanのoptimizeを用いた最尤推定のパラメータリカバリをします。αは0.1から1の範囲で0.1刻み，βは0.5から5の範囲で0.5刻みでデータ生成とパラメータ推定を行います(つまり100個分チェックします)。

```
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL
beta_set <- 0

for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      q_learning_mle <- q_learning$optimize(
      data = list(trial = nrow(data_2),
              reward = data_2$reward,
              choice = data_2$choice + 1),
      seed = 123)
      alpha_estimated[(i-1)*10 + j] <- q_learning_mle$mle("alpha")
      beta_estimated[(i-1)*10 + j] <- q_learning_mle$mle("beta")
    },error = function(e) {message(e)})
  }
}

parameter_recovery_mle <- data.frame(alpha_true = alpha_true[1:length(alpha_estimated)],
                                     beta_true = beta_true[1:length(alpha_estimated)],
                                     alpha_estimated = alpha_estimated[1:length(alpha_estimated)],
                                     beta_estimated = beta_estimated[1:length(alpha_estimated)])
```


```{r message=FALSE, warning=FALSE, include=FALSE}
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL
beta_set <- 0

for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      q_learning_mle <- q_learning$optimize(
      data = list(trial = nrow(data_2),
              reward = data_2$reward,
              choice = data_2$choice + 1),
      seed = 123)
      alpha_estimated[(i-1)*10 + j] <- q_learning_mle$mle("alpha")
      beta_estimated[(i-1)*10 + j] <- q_learning_mle$mle("beta")
    },error = function(e) {message(e)})
  }
}

parameter_recovery_mle <- data.frame(alpha_true = alpha_true[1:length(alpha_estimated)],
                                     beta_true = beta_true[1:length(alpha_estimated)],
                                     alpha_estimated = alpha_estimated[1:length(alpha_estimated)],
                                     beta_estimated = beta_estimated[1:length(alpha_estimated)])
```

#### パラメータリカバリ（最尤推定）の確認

パラメータリカバリーのチェックをしましょう。散布図を書いて，真値（研究者がデータ生成時に設定した値）と推定された値が強い相関を示しているか確認します（データ生成時や推定時に確率的な変動が生じるので，完全一致はありません）。最尤推定だと，一部のパラメータは推定ミスをすることがあります。

```{r}
parameter_recovery_mle %>% 
  ggplot(aes(x = alpha_true, y = alpha_estimated)) +
  geom_point()
```


どうもβの推定値の中にとても大きな値になってしまったものがあるようです。βの最大値は制約をかけてないので，すごく大きくなることがあります。


```{r}
parameter_recovery_mle %>% 
  ggplot(aes(x = beta_true, y = beta_estimated)) +
  geom_point()
```

確認しにくいので，10以下の推定値のみをプロットして確認します。

```{r}
parameter_recovery_mle %>%
  filter(beta_estimated < 10) %>% 
  ggplot(aes(x = beta_true, y = beta_estimated)) +
  geom_point()
```

### ベイズ推定

- 今度は，MCMCを用いたベイズ推定をしてみましょう！最尤推定のときは，optimizeでしたが，今度は，sampleを使います。違う点としては，MCMCの連鎖（チェーン）の本数（chains 4本が良いと思います），MCMCの初期のあまり収束してい部分をどのくらい捨てるか(iter_warmup, モデルに依存するのでtrace plotを見てから判断するのが良いと思います）,MCMCサンプルの数(iter_sampling 多いほうが推定は安定しますが，時間がかかります)，　並列化で使用するコアの数（parallel_chains，MCMCのチェーンを４とした場合は，４がいいですが，お手持ちのパソコンのコア数に依存します）を指定する点です。


```
q_learning_mcmc <- q_learning$sample(
  data = list(trial = nrow(data_1),
              reward = data_1$reward,
              choice = data_1$choice + 1),
  seed = 123,
  chains = 4,
  iter_warmup = 500,
  iter_sampling = 1000,
  parallel_chains = 4)
```


```{r include=FALSE}
q_learning_mcmc  <- q_learning$sample(
  data = list(trial = nrow(data_1),
              reward = data_1$reward,
              choice = data_1$choice + 1),
  seed = 123,
  chains = 4,
  iter_warmup = 500,
  iter_sampling = 1000,
  parallel_chains = 4)
```

結果の要約を確認してみましょう。

```{r}
q_learning_mcmc$summary()
```

以下はMCMCについての診断結果です。

```{r}
q_learning_mcmc$cmdstan_diagnose()
```

trace plot と事後分布をプロットしてみましょう。trace plotはMCMCのチェーンがきれいにまざっているか確認をしましょう。

```{r}
# ggplotやtidyverseで扱いやすく処理する
mcmc_samples = as_draws_df(q_learning_mcmc$draws())
# alphaのtrace plot
mcmc_samples %>%
  mutate(chain = as.factor(.chain)) %>% 
  ggplot(aes(x = .iteration, y = alpha, group = .chain, color = chain)) +
  geom_line()
# betaのtrace plot(ごく少数の大きな値でプロットできないので10以下に絞った)
mcmc_samples %>%
  mutate(chain = as.factor(.chain)) %>% 
  filter(beta < 10) %>% 
  ggplot(aes(x = .iteration, y = beta, group = .chain, color = chain)) +
  geom_line()
# alphaの事後分布
mcmc_samples %>% 
  ggplot() +
  geom_histogram(aes(x=alpha),binwidth = 0.01)
# betaの事後分布（ごく少数の大きな値があるとプロットできないので6以下に絞った）
mcmc_samples %>% 
  filter(beta < 6) %>% 
ggplot() +
  geom_histogram(aes(x=beta),binwidth = 0.1)
```

### パラメータリカバリ（ベイズ推定）

ベイズ推定でもパラメータリカバリをしてみましょう。ただ，これは結構計算に時間がかかると思います。

```
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL
beta_set <- 0

for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    print(paste("進捗状況：",(i-1)*10 + j,"/100"))
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      q_learning_mcmc <- q_learning$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)
      mcmc_samples = as_draws_df(q_learning_mcmc$draws())
      alpha_estimated[(i-1)*10 + j] <- mean(mcmc_samples$alpha)
      beta_estimated[(i-1)*10 + j] <- mean(mcmc_samples$beta)
    },error = function(e) {message(e)})
  }
}

parameter_recovery_mcmc <- data.frame(alpha_true = alpha_true,
                                     beta_true = beta_true,
                                     alpha_estimated = alpha_estimated,
                                     beta_estimated = beta_estimated)
```


```{r include=FALSE}
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL
beta_set <- 0
for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    print(paste("進捗状況：",(i-1)*10 + j,"/100"))
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      q_learning_mcmc <- q_learning$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)
      mcmc_samples = as_draws_df(q_learning_mcmc$draws())
      alpha_estimated[(i-1)*10 + j] <- mean(mcmc_samples$alpha)
      beta_estimated[(i-1)*10 + j] <- mean(mcmc_samples$beta)
    },error = function(e) {message(e)})
  }
}

parameter_recovery_mcmc <- data.frame(alpha_true = alpha_true,
                                     beta_true = beta_true,
                                     alpha_estimated = alpha_estimated,
                                     beta_estimated = beta_estimated)
```

#### パラメータリカバリ（ベイズ推定）の確認

パラメータリカバリーのチェックをしましょう。散布図を書いて，真値（研究者がデータ生成時に設定した値）と推定された値が強い相関を示しているか確認します（データ生成時や推定時に確率的な変動が生じるので，完全一致はありません）。

```{r}
parameter_recovery_mcmc %>% 
  ggplot(aes(x = alpha_true, y = alpha_estimated)) +
  geom_point()
```

最尤推定同様に，βがすごく大きくなることがあるのは，気になるところです。

```{r}
parameter_recovery_mcmc %>%
  ggplot(aes(x = beta_true, y = beta_estimated)) +
  geom_point()
```

### 事前分布の活用

- 上記の無情報の事前分布の場合だと，ベータがとても大きな値になる点が気にかかるところです。そこで事前分布にも情報をもたせることにします。

- αについては，0や１に近い値を取ることは稀であると考えられるので，以下のベータ分布beta(2,2)のような形状だと推定が安定するかもしれません。

```{r}
alpha = seq(0,1, length=100)
plot(alpha, dbeta(alpha, 2, 2), ylab="density", type ="l", col=4)
```

βは極端に大きな値をとりましたが，βがあまりに大きくても意味はないので，裾広がりなガンマ分布(gamma(2,0.5))を使うことで，極端に大きな推定値を避けられるかもしれません。

```{r}
beta = seq(0,15, length=500)
plot(beta, dgamma(beta, 2, 0.5), ylab="density", type ="l", col=4)
```

### 情報の有る事前分布を用いた場合のパラメータリカバリ

上記を踏まえて，αの事前分布にベータ分布，ベータの事前分布にガンマ分布を仮定したStanコードを書いて，q_learning_prior.stanという名前で保存します。

```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}
parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
  real<lower=0.0> beta;            //逆温度
}
model {
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  //学習率の事前分布にベータ分布，逆温度の事前分布にガンマ分布
  alpha ~ beta(2, 2); 
  beta ~ gamma(2, 0.5);
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}
```

q_learning_prior.stanを使って，パラメータリカバリを実施してみます。

まず，上記のモデルをコンパイルします。

```{r include=FALSE}
q_learning_prior <- cmdstan_model('rl/q_learning_prior.stan')
```

```
q_learning_prior <- cmdstan_model('q_learning_prior.stan')
```

まず，先程のdata_1で試してみます。

```{r include=FALSE}
q_learning_prior_mcmc <- q_learning_prior$sample(
  data = list(trial = nrow(data_1),
              reward = data_1$reward,
              choice = data_1$choice + 1),
  seed = 123,
  chains = 4,
  iter_warmup = 500,
  iter_sampling = 1000,
  parallel_chains = 4)
```

```
q_learning_prior_mcmc <- q_learning_prior$sample(
  data = list(trial = nrow(data_1),
              reward = data_1$reward,
              choice = data_1$choice + 1),
  seed = 123,
  chains = 4,
  iter_warmup = 500,
  iter_sampling = 1000,
  parallel_chains = 4)
```

結果の要約を確認してみましょう。

```{r}
q_learning_prior_mcmc$summary()
```

以下はMCMCについての診断結果です。収束診断の問題もないかと思います。

```{r}
q_learning_prior_mcmc$cmdstan_diagnose()
```



それでは，先程と同じデータでパラメータリカバリーをしてみましょう！

```
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL
beta_set <- 0

for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    print(paste("進捗状況：",(i-1)*10 + j,"/100"))
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      q_learning_prior_mcmc <- q_learning_prior$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)
      mcmc_samples = as_draws_df(q_learning_prior_mcmc$draws())
      alpha_estimated[(i-1)*10 + j] <- mean(mcmc_samples$alpha)
      beta_estimated[(i-1)*10 + j] <- mean(mcmc_samples$beta)
    },error = function(e) {message(e)})
  }
}

parameter_recovery_prior_mcmc <- data.frame(alpha_true = alpha_true,
                                     beta_true = beta_true,
                                     alpha_estimated = alpha_estimated,
                                     beta_estimated = beta_estimated)
```


```{r include=FALSE}
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL
beta_set <- 0

for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    print(paste("進捗状況：",(i-1)*10 + j,"/100"))
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      q_learning_prior_mcmc <- q_learning_prior$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)
      mcmc_samples = as_draws_df(q_learning_prior_mcmc$draws())
      alpha_estimated[(i-1)*10 + j] <- mean(mcmc_samples$alpha)
      beta_estimated[(i-1)*10 + j] <- mean(mcmc_samples$beta)
    },error = function(e) {message(e)})
  }
}

parameter_recovery_prior_mcmc <- data.frame(alpha_true = alpha_true,
                                     beta_true = beta_true,
                                     alpha_estimated = alpha_estimated,
                                     beta_estimated = beta_estimated)
```


#### 有情報事前分布を用いた場合のパラメータリカバリの確認

いい感じにパラメータリカバリーできています。無情報事前分布のときのようにすごく大きなβが発生しなくなっています。

```{r}
parameter_recovery_prior_mcmc %>% 
  ggplot(aes(x = alpha_true, y = alpha_estimated)) +
  geom_point()
parameter_recovery_prior_mcmc %>%
  ggplot(aes(x = beta_true, y = beta_estimated)) +
  geom_point()
```


## C データ収集と行動データを確認

「強化学習モデルを使ったモデル・フィッティング１」と同じデータを使います。以下のコードで５名分のlong形式のデータセットを準備します。

```{r message=FALSE, warning=FALSE}
setwd("data")
file_names <- list.files()
setwd("..")
# 確認用の図を入れる場所を確保
plot_check <- NULL
# データを入れる場所を確保
data_long <- NULL
for (i in 1:length(file_names)) {
  # file_namesのi番目のデータを読み込んで,上記の処理をして，tmp_dataに格納
  tmp_data <- read_csv(paste("data",file_names[i], sep = "/")) %>% 
    filter(trial_type == "html-button-response") %>% 
    mutate(id = rep(i,80),
           trial = 1:80, 
           s1_prob = rep(c(0.2,0.8),each = 40),
           reward = ifelse(button_pressed == 0, reward_s1, reward_s2)) %>% 
    select(id, trial,choice=button_pressed, rt, reward, s1_prob,reward_s1, reward_s2)
  # データの保存
  data_long <- rbind(data_long, tmp_data)
  
  # plot
  plot_check[[i]] <- ggplot(tmp_data, aes(x = trial, y = s1_prob)) +
    geom_line() +
    geom_line(aes(x= trial, y = choice), colour = 'blue') +
    geom_point(aes(x = trial, y = reward),colour = 'red')
}
```

## D パラメータ推定

### 個人のパラメータ推定（無情報事前分布）

Sub01

```
data_individual <- data_long %>% 
  filter(id == 1)

q_learning_mcmc <- q_learning$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)

q_learning_mcmc$summary()
q_learning_mcmc$cmdstan_diagnose()
```

```{r include=FALSE}
data_individual <- data_long %>% 
  filter(id == 1)

q_learning_mcmc <- q_learning$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)

q_learning_mcmc$summary()
q_learning_mcmc$cmdstan_diagnose()
```

結果のプロット

```{r}
q_learning_mcmc <- as_draws_df(q_learning_mcmc$draws())
# alphaのtrace plot
q_learning_mcmc %>%
  mutate(chain = as.factor(.chain)) %>% 
  ggplot(aes(x = .iteration, y = alpha, group = .chain, color = chain)) +
  geom_line()
# betaのtrace plot
q_learning_mcmc %>%
  mutate(chain = as.factor(.chain)) %>% 
  filter(beta < 1000) %>% 
  ggplot(aes(x = .iteration, y = beta, group = .chain, color = chain)) +
  geom_line()
# alphaの事後分布
q_learning_mcmc %>% 
  ggplot() +
  geom_histogram(aes(x=alpha),binwidth = 0.01)
# betaの事後分布
q_learning_mcmc %>% 
  filter(beta < 1000) %>% 
ggplot() +
  geom_histogram(aes(x=beta),binwidth = 0.1)
q_values <- q_learning_ll(mean(mcmc_samples$alpha),mean(mcmc_samples$beta),data_individual)
```

### 個人のパラメータ推定（有情報事前分布）

sub01

```
data_individual <- data_long %>% 
  filter(id == 1)

q_learning_prior_mcmc <- q_learning_prior$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)

q_learning_prior_mcmc$summary()
q_learning_prior_mcmc$cmdstan_diagnose()
```


```{r include=FALSE}
data_individual <- data_long %>% 
  filter(id == 1)

q_learning_prior_mcmc <- q_learning_prior$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)

q_learning_prior_mcmc$summary()
q_learning_prior_mcmc$cmdstan_diagnose()
```

結果のプロット

```{r}
q_learning_prior_mcmc <- as_draws_df(q_learning_prior_mcmc$draws())
# alphaのtrace plot
q_learning_prior_mcmc %>%
  mutate(chain = as.factor(.chain)) %>% 
  ggplot(aes(x = .iteration, y = alpha, group = .chain, color = chain)) +
  geom_line()
# betaのtrace plot
q_learning_prior_mcmc %>%
  mutate(chain = as.factor(.chain)) %>% 
  ggplot(aes(x = .iteration, y = beta, group = .chain, color = chain)) +
  geom_line()
# alphaの事後分布
q_learning_prior_mcmc %>% 
  ggplot() +
  geom_histogram(aes(x=alpha),binwidth = 0.01)
# betaの事後分布
q_learning_prior_mcmc %>% 
ggplot() +
  geom_histogram(aes(x=beta),binwidth = 0.1)
q_values <- q_learning_ll(mean(mcmc_samples$alpha),mean(mcmc_samples$beta),data_individual)
```


さて，ここまでで，ベイズ推定が出来るようになりました。次は<a href="https://kunisatolab.github.io/main/how-to-cognitive-modeling-rl3.html" target="_blank">「強化学習モデル: ベイズ推定(2)」</a>で，モデル比較などに取り組みます。


## 引用・参考文献

- Busemeyer, J. R., & Diederich, A. (2010). Cognitive Modeling. SAGE.
- Heathcote, A., Brown, S. D., & Wagenmakers, E.-J. (2015). An Introduction to Good Practices in Cognitive Modeling. In B. U. Forstmann & E.-J. Wagenmakers (Eds.), An Introduction to Model-Based Cognitive Neuroscience (pp. 25–48). Springer New York.
- Palminteri, S., Wyart, V., & Koechlin, E. (2017). The Importance of Falsification in Computational Cognitive Modeling. Trends in Cognitive Sciences, 21(6), 425–433.