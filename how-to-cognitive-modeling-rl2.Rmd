---
title: "強化学習モデル2"
---

# 認知モデリングの推奨実践法

- Busemeyer & Diederich(2010), Heathcote (2014), Palminteri et al.(2017)を参考にまとめると，以下のような感じになります。

- A 認知課題と認知モデルを準備
- B 人工データ生成とパラメータリカバリーを確認（モデルや課題の修正）
- C データ収集と行動データを確認
- D パラメータ推定
- E 相対モデル比較
- F モデル・シミュレーションと選択的影響テスト

以降は，上記に従って進めていきます。「強化学習モデルを使ったモデル・フィッティング１」ではABCD（一部F）とすすめてきましたが，ここで，推定をcmdstanを使って行いますので，一度Bに戻ってからDEFと進みます。

# 使用するRパッケージ

```{r message=FALSE, warning=FALSE}
rm(list = ls())
library(tidyverse)
library(cmdstanr)
library(rstan)
library(posterior)
library(bridgesampling)
```


# B 人工データ生成とパラメータリカバリーを確認（モデルや課題の修正）

## 人口データの生成

改めて人工データを生成するために，実験状況の入ったsim_dataの準備とq_learning_sim関数とq_learning_ll関数の準備をします。

```{r, echo=TRUE, message=TRUE, warning=TRUE}
sim_data <- tibble(trial = 1:80,
                   prob_s1 = rep(c(0.2, 0.8), each = 40),
                   prob_s2 = rep(c(0.8, 0.2), each = 40),
                   reward_s1 = ifelse(runif(80) < prob_s1, 1, 0),
                   reward_s2 = ifelse(runif(80) < prob_s2, 1, 0))

q_learning_sim <- function(alpha, beta,data) {
  #変数の準備
  value_s1 <- 0          # s1の価値(初期値は0)
  value_s2 <- 0          # s2の価値(初期値は0)
  current_choice <- NULL     # ある時点の選択（1=s1，0=s2）
  choice_prob_s1 <- NULL  # s1の選択確率
  reward <- NULL                 # 報酬
  # Qlearningモデル
  for (i in 1:nrow(data)){
    # s1を選ぶ確率を計算し,一様分布から発生させた乱数が行動選択確率よりも小さい時に1（s1），大きい時に0（s2）
    choice_prob_s1[i] <- exp(beta*value_s1[i])/(exp(beta*value_s1[i])+exp(beta*value_s2[i]))
    current_choice[i] <- as.integer(runif(1,min=0,max=1) <= choice_prob_s1[i])
    #FBを報酬(r)として、価値の更新を行う。
    if (current_choice[i] == 1){
        reward[i] <- data$reward_s1[i]
        #予測誤差の計算
        prediction_error <-  reward[i] - value_s1[i]
        #予測誤差を使ってs1の価値を更新する
        value_s1[i+1] <- value_s1[i]+alpha*prediction_error
        #s2は更新なし
        value_s2[i+1] <- value_s2[i]
    }else{
        reward[i] <- data$reward_s2[i]
        #予測誤差の計算
        prediction_error <- reward[i] - value_s2[i]
        #予測誤差を使ってs2の価値を更新する
        value_s2[i+1] <- value_s2[i]+alpha*prediction_error
        #s1は更新なし
        value_s1[i+1] <- value_s1[i]
    }
  }
  result <- data.frame(trial = data$trial,
              value_s1 = value_s1[1:nrow(data)], 
              value_s2 = value_s2[1:nrow(data)], 
              prob_s1 = choice_prob_s1,
              choice = current_choice,
              reward = reward)
  return(result)
}

q_learning_ll <- function(alpha, beta,data) {
  #変数の準備
  value_s1 <- 0          # s1の価値(初期値は0)
  value_s2 <- 0          # s2の価値(初期値は0)
  prob_s1 <- NULL  # s1の選択確率
  ll <- 0                 # 対数尤度
  # Qlearningモデル
  for (i in 1:nrow(data)){
    # s1を選ぶ確率を計算
    prob_s1[i] <- exp(beta*value_s1[i])/(exp(beta*value_s1[i])+exp(beta*value_s2[i]))
    #FBを報酬(r)として、価値の更新を行う。
    if (data$choice[i] == 1){
        #予測誤差の計算
        prediction_error <-  data$reward[i] - value_s1[i]
        #予測誤差を使ってs1の価値を更新する
        value_s1[i+1] <- value_s1[i]+alpha*prediction_error
        #s2は更新なし
        value_s2[i+1] <- value_s2[i]
        # 対数尤度の計算のために選択したs1を選ぶ確率の対数を加算
        ll <- ll + log(prob_s1[i])
    }else{
        #予測誤差の計算
        prediction_error <- data$reward[i] - value_s2[i]
        #予測誤差を使ってs2の価値を更新する
        value_s2[i+1] <- value_s2[i]+alpha*prediction_error
        #s1は更新なし
        value_s1[i+1] <- value_s1[i]
        # 対数尤度の計算のために選択したs2を選ぶ確率の対数を加算
        ll <- ll + log(1-prob_s1[i])
    }
  }
  result <- data.frame(trial = data$trial,
              value_s1 = value_s1[1:nrow(data)], 
              value_s2 = value_s2[1:nrow(data)], 
              prob_s1 = prob_s1,
              choice = data$choice,
              reward = data$reward)
  return(list(result = result, ll = ll))
}
```

alpha = 0.3, beta = 2で，シミュレーション・データを生成します。

```{r}
data_1 <- q_learning_sim(alpha = 0.3, beta = 2, sim_data)
```

## cmdstanrでの推定
### Stanコードの作成

"q_learning_non_info_single.stan"というファイルを作成して，以下のコードを書き込みます。Stanは統計モデリング用のプラットフォームで，MCMCサンプリングによるベイズ推定，変分推定，最適化による最尤推定が可能です。Stanでは，data,parameters,modelのようにブロックごとに指定をして，書いていきます（この３つが最小限のブロック数かと思います）。dataブロックでは入力するデータについて記述します（型と範囲の指定が必要です）。parameterブロックでは，推定するパラメータについて記述します（型と範囲の指定が必要です）。modelブロックでは，データとパラメータを用いた生成モデルの記載をします。なお，データ~分布(y ~ normal(mu, sig))のような形で記述もできますし，target += 対数尤度のように，対数尤度でも記述ができます。以下は，Q学習の推定用のコードですが，グリッドサーチで用意したものに比べて，すっきりしているかと思います。

```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}

parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
  real<lower=0.0> beta;            //逆温度
}

model {
  //学習率と逆温度の事前分布の指定はしていないので，parametersで指定した範囲の無情報事前分布が使われる
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}
```

Stanコードがstanファイルとして保存できたら，cmdstan_model()コンパイルします。Rは便利ですが計算は遅い言語なので，c++のような高速な計算が可能な言語にコンパイルをします。ちなみに，stanをRで使う場合は，Rstanを使うことが多かったのですが，最近は，cmdstanrが開発されており，こっちのほうがコンパイルもサンプリングも早いのでおすすめです。

```{r}
q_non_info_single <- cmdstan_model('rl/q_learning_non_info_single.stan')
```

## 最尤推定
コンパイルができたら，最適化による最尤推定をしてみましょう。これまで国里が作ったシンプルなグリッドサーチでしたが，より洗練した方法で最尤推定値を推定します。コンパイルしたモデル$optimize()で推定ができます。必要なのは，データだけです（一応シードは123で指定しています）。グリッドサーチよりも素早く計算できると思います。

```{r}
mle_cmdstan <- q_non_info_single$optimize(
  data = list(trial = nrow(data_1),
              reward = data_1$reward,
              choice = data_1$choice + 1),
  seed = 123)
mle_cmdstan$summary()
```


## パラメータリカバリ（最尤推定）

グリッドサーチは時間がかかることもあって，ちゃんとしたパラメータリカバリしなかったのですが，ここで，最尤推定のパラメータリカバリをします。αは0.1から1の範囲で0.1刻み，βは0.5から5の範囲で0.5刻みでデータ生成とパラメータ推定を行います(つまり100個分チェックします)。

```{r message=FALSE, warning=FALSE}
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL

beta_set <- 0
parameter_recovery_mle <- NULL
for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      mle_cmdstan <- q_non_info_single$optimize(
      data = list(trial = nrow(data_2),
              reward = data_2$reward,
              choice = data_2$choice + 1),
      seed = 123)
      alpha_estimated[(i-1)*10 + j] <- mle_cmdstan$mle("alpha")
      beta_estimated[(i-1)*10 + j] <- mle_cmdstan$mle("beta")
    },error = function(e) {message(e)})
  }
}


parameter_recovery_mle <- data.frame(alpha_true = alpha_true[1:length(alpha_estimated)],
                                     beta_true = beta_true[1:length(alpha_estimated)],
                                     alpha_estimated = alpha_estimated[1:length(alpha_estimated)],
                                     beta_estimated = beta_estimated[1:length(alpha_estimated)])
parameter_recovery_mle
```

### パラメータリカバリ（最尤推定）の確認

パラメータリカバリーのチェックをしましょう。散布図を書いて，真値（研究者がデータ生成時に設定した値）と推定された値が強い相関を示しているか確認します（データ生成時や推定時に確率的な変動が生じるので，完全一致はありません）。最尤推定だと，一部のパラメータは推定ミスをすることがあります。

```{r}
parameter_recovery_mle %>% 
  ggplot(aes(x = alpha_true, y = alpha_estimated)) +
  geom_point()

parameter_recovery_mle %>% 
  ggplot(aes(x = beta_true, y = beta_estimated)) +
  geom_point()
```

βの最大値は制約をかけてないので，すごく大きくなることがあります。確認しにくいので，10以下の推定値のみをプロットして確認します。

```{r}
parameter_recovery_mle %>%
  filter(beta_estimated < 10) %>% 
  ggplot(aes(x = beta_true, y = beta_estimated)) +
  geom_point()
```

## ベイズ推定

- 今度は，MCMCを用いたベイズ推定をしてみましょう！最尤推定のときは，optimizeでしたが，今度は，sampleを使います。違う点としては，MCMCの連鎖（チェーン）の本数（chains 4本が良いと思います），MCMCの初期のあまり収束してい部分をどのくらい捨てるか(iter_warmup, モデルに依存するのでtrace plotを見てから判断するのが良いと思います）,MCMCサンプルの数(iter_sampling 多いほうが推定は安定しますが，時間がかかります)，　並列化で使用するコアの数（parallel_chains MCMCチェーンを４とした場合は，４がいいですが，お手持ちのパソコンのコア数によります）

- summary()で推定値の要約を確認できます。cmdstan_diagnose()で収束の判定を確認できます。

```{r}
q_non_info_single_cmdstan <- q_non_info_single$sample(
  data = list(trial = nrow(data_1),
              reward = data_1$reward,
              choice = data_1$choice + 1),
  seed = 123,
  chains = 4,
  iter_warmup = 500,
  iter_sampling = 1000,
  parallel_chains = 4)

q_non_info_single_cmdstan$summary()
q_non_info_single_cmdstan$cmdstan_diagnose()
```

trace plot と事後分布をプロットしてみましょう。trace plotはMCMCのチェーンがきれいにまざっているか確認をしましょう。

```{r}
# ggplotやtidyverseで扱いやすく処理する
mcmc_samples = as_draws_df(q_non_info_single_cmdstan$draws())

# alphaのtrace plot
mcmc_samples %>%
  mutate(chain = as.factor(.chain)) %>% 
  ggplot(aes(x = .iteration, y = alpha, group = .chain, color = chain)) +
  geom_line()

# betaのtrace plot(ごく少数の大きな値でプロットできないので10以下に絞った)
mcmc_samples %>%
  mutate(chain = as.factor(.chain)) %>% 
  filter(beta < 10) %>% 
  ggplot(aes(x = .iteration, y = beta, group = .chain, color = chain)) +
  geom_line()

# alphaの事後分布
mcmc_samples %>% 
  ggplot() +
  geom_histogram(aes(x=alpha),binwidth = 0.01)

# betaの事後分布（ごく少数の大きな値があるとプロットできないので6以下に絞った）
mcmc_samples %>% 
  filter(beta < 6) %>% 
ggplot() +
  geom_histogram(aes(x=beta),binwidth = 0.1)
```

## パラメータリカバリ（ベイズ推定）

ベイズ推定でもパラメータリカバリをしてみましょう。ただ，これは結構長くなるので，実際には実行しなくても大丈夫です。

```{r message=FALSE, warning=FALSE}
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL

beta_set <- 0
parameter_recovery_mle <- NULL
for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    print(paste("進捗状況：",(i-1)*10 + j,"/100"))
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      mcmc_cmdstan <- q_non_info_single$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)
      mcmc_samples = as_draws_df(mcmc_cmdstan$draws())
      alpha_estimated[(i-1)*10 + j] <- mean(mcmc_samples$alpha)
      beta_estimated[(i-1)*10 + j] <- mean(mcmc_samples$beta)
    },error = function(e) {message(e)})
  }
}

parameter_recovery_mcmc <- data.frame(alpha_true = alpha_true,
                                     beta_true = beta_true,
                                     alpha_estimated = alpha_estimated,
                                     beta_estimated = beta_estimated)
```

### パラメータリカバリ（ベイズ推定）の確認

パラメータリカバリーのチェックをしましょう。散布図を書いて，真値（研究者がデータ生成時に設定した値）と推定された値が強い相関を示しているか確認します（データ生成時や推定時に確率的な変動が生じるので，完全一致はありません）。最尤推定だと，一部のパラメータは推定ミスをすることがあります。βがすごく大きくなることがあるのはかなり気になるところです。

```{r}
parameter_recovery_mcmc %>% 
  ggplot(aes(x = alpha_true, y = alpha_estimated)) +
  geom_point()

parameter_recovery_mcmc %>%
  filter(beta_estimated < 10) %>% 
  ggplot(aes(x = beta_true, y = beta_estimated)) +
  geom_point()
```

### 弱情報事前分布の活用

- 上記の無情報の事前分布の場合だと，ベータがとても大きな値になる点が気にかかるところです。そこで事前分布にも情報をもたせることにします。

- αについては，0や１に近い値を取ることは稀であると考えられるので，以下のベータ分布beta(2,2)のような形状だと推定が安定するかもしれません。

```{r}
alpha = seq(0,1, length=100)
plot(alpha, dbeta(alpha, 2, 2), ylab="density", type ="l", col=4)
```

βは極端に大きな値をとりましたが，βがあまりに大きくても意味はないので，裾広がりなガンマ分布(gamma(2,0.5))を使うことで，極端に大きな推定値を避けられるかもしれません。

```{r}
beta = seq(0,15, length=500)
plot(beta, dgamma(beta, 2, 0.5), ylab="density", type ="l", col=4)
```

### 弱情報事前分布を用いた場合のパラメータリカバリ

上記を踏まえて，αの事前分布にベータ分布，ベータの事前分布にガンマ分布を仮定したStanコードを書いて，q_learning_weak_info_single.stanという名前で保存します。

```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}

parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
  real<lower=0.0> beta;            //逆温度
}

model {
  //学習率と逆温度の事前分布の指定はしていないので，parametersで指定した範囲の無情報事前分布が使われる
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  //学習率の事前分布にベータ分布，逆温度の事前分布にガンマ分布
  alpha ~ beta(2, 2); 
  beta ~ gamma(2, 0.5);
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}
```

q_learning_weak_info_single.stanを使って，パラメータリカバリを実施してみます。

```{r message=FALSE, warning=FALSE}
# モデルのコンパイル
q_weak_info_single <- cmdstan_model('rl/q_learning_weak_info_single.stan')
alpha_true <- NULL
beta_true <- NULL
alpha_estimated <- NULL
beta_estimated <- NULL

beta_set <- 0
parameter_recovery_mle <- NULL
for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    alpha_true[(i-1)*10 + j] <- alpha_set
    beta_true[(i-1)*10 + j] <- beta_set
    print(paste("進捗状況：",(i-1)*10 + j,"/100"))
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      mcmc_cmdstan <- q_weak_info_single$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)
      mcmc_samples = as_draws_df(mcmc_cmdstan$draws())
      alpha_estimated[(i-1)*10 + j] <- mean(mcmc_samples$alpha)
      beta_estimated[(i-1)*10 + j] <- mean(mcmc_samples$beta)
    },error = function(e) {message(e)})
  }
}

parameter_recovery_mcmc2 <- data.frame(alpha_true = alpha_true,
                                     beta_true = beta_true,
                                     alpha_estimated = alpha_estimated,
                                     beta_estimated = beta_estimated)
```


### 弱情報事前分布を用いた場合のパラメータリカバリの確認

いい感じにパラメータリカバリーできています。無情報事前分布のときのようにすごく大きなβが発生しなくなっています。

```{r}
parameter_recovery_mcmc2 %>% 
  ggplot(aes(x = alpha_true, y = alpha_estimated)) +
  geom_point()

parameter_recovery_mcmc2 %>%
  ggplot(aes(x = beta_true, y = beta_estimated)) +
  geom_point()
```

# C データ収集と行動データを確認

「強化学習モデルを使ったモデル・フィッティング１」と同じデータを使います。以下のコードで５名分のlong形式のデータセットを準備します。

```{r message=FALSE, warning=FALSE}
setwd("rl/data")
file_names <- list.files()
setwd("..")
setwd("..")
# 確認用の図を入れる場所を確保
plot_check <- NULL
# データを入れる場所を確保
data_long <- NULL

for (i in 1:length(file_names)) {
  # file_namesのi番目のデータを読み込んで,上記の処理をして，tmp_dataに格納
  tmp_data <- read_csv(paste("rl/data",file_names[i], sep = "/")) %>% 
    filter(trial_type == "html-button-response") %>% 
    mutate(id = rep(i,80),
           trial = 1:80, 
           s1_prob = rep(c(0.2,0.8),each = 40),
           reward = ifelse(button_pressed == 0, reward_s1, reward_s2)) %>% 
    select(id, trial,choice=button_pressed, rt, reward, s1_prob,reward_s1, reward_s2)
  # データの保存
  data_long <- rbind(data_long, tmp_data)
  
  # plot
  plot_check[[i]] <- ggplot(tmp_data, aes(x = trial, y = s1_prob)) +
    geom_line() +
    geom_line(aes(x= trial, y = choice), colour = 'blue') +
    geom_point(aes(x = trial, y = reward),colour = 'red')
}
```

# D パラメータ推定

## 個人のパラメータ推定（無情報事前分布）

Sub01

```{r}
# 特定の個人のデータを抽出
data_individual <- data_long %>% 
  filter(id == 1)

mcmc_cmdstan <- q_non_info_single$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)

q_non_info_single_cmdstan$summary()
q_non_info_single_cmdstan$cmdstan_diagnose()
```

結果のプロット

```{r}
mcmc_samples <- as_draws_df(mcmc_cmdstan$draws())
# alphaのtrace plot
mcmc_samples %>%
  mutate(chain = as.factor(.chain)) %>% 
  ggplot(aes(x = .iteration, y = alpha, group = .chain, color = chain)) +
  geom_line()

# betaのtrace plot(ごく少数の大きな値でプロットできないので10以下に絞った)
mcmc_samples %>%
  mutate(chain = as.factor(.chain)) %>% 
  filter(beta < 1000) %>% 
  ggplot(aes(x = .iteration, y = beta, group = .chain, color = chain)) +
  geom_line()

# alphaの事後分布
mcmc_samples %>% 
  ggplot() +
  geom_histogram(aes(x=alpha),binwidth = 0.01)

# betaの事後分布（ごく少数の大きな値があるとプロットできないので6以下に絞った）
mcmc_samples %>% 
  filter(beta < 1000) %>% 
ggplot() +
  geom_histogram(aes(x=beta),binwidth = 0.1)

q_values <- q_learning_ll(mean(mcmc_samples$alpha),mean(mcmc_samples$beta),data_individual)
```

## 個人のパラメータ推定（弱情報事前分布）

sub01

```{r}
# 特定の個人のデータを抽出
data_individual <- data_long %>% 
  filter(id == 1)

mcmc_cmdstan <- q_weak_info_single$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 1000,
        parallel_chains = 4)

q_non_info_single_cmdstan$summary()
q_non_info_single_cmdstan$cmdstan_diagnose()
```

結果のプロット

```{r}
mcmc_samples <- as_draws_df(mcmc_cmdstan$draws())
# alphaのtrace plot
mcmc_samples %>%
  mutate(chain = as.factor(.chain)) %>% 
  ggplot(aes(x = .iteration, y = alpha, group = .chain, color = chain)) +
  geom_line()

# betaのtrace plot
mcmc_samples %>%
  mutate(chain = as.factor(.chain)) %>% 
  ggplot(aes(x = .iteration, y = beta, group = .chain, color = chain)) +
  geom_line()

# alphaの事後分布
mcmc_samples %>% 
  ggplot() +
  geom_histogram(aes(x=alpha),binwidth = 0.01)

# betaの事後分布
mcmc_samples %>% 
ggplot() +
  geom_histogram(aes(x=beta),binwidth = 0.1)

q_values <- q_learning_ll(mean(mcmc_samples$alpha),mean(mcmc_samples$beta),data_individual)
```

## 他のデータでもベイズ推定してみよう！

# E 相対モデル比較

sub01のデータの推定を考えると，弱情報事前分布の方が良さそうに思えますが，ここはモデル比較をしてみたいと思います。モデル比較する場合には，以下の２種類があります。今回は，予測をしたいという話ではないので，WBICを使用します。

- 予測の良さに基づくモデル選択法：AIC, 交差検証法，WAIC

- ベイズ的なモデル選択法(モデルと事前分布からみてどのくらいデータが意外か。この意外性が小さいほど良いモデルと考えられる)：BIC, WBIC, ベイズファクター,自由エネルギー


WBICの計算をする場合は，stanコードの対数尤度の計算の部分で対数尤度に(1/log(データ数)) を掛ける必要があります。さらに，generated quantitiesブロックで対数尤度を計算します。それぞれは，以下になります。

- q_learning_non_info_single_WBIC.stan

```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}

parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
  real<lower=0.0> beta;            //逆温度
}

model {
  //学習率と逆温度の事前分布の指定はしていないので，parametersで指定した範囲の無情報事前分布が使われる
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += (1/log(trial))*log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}

generated quantities {
  vector[trial] log_lik;
  {
    matrix[trial, 2] Q;
    Q[1, 1] = 0;
    Q[1, 2] = 0;
    for ( t in 1:trial ) {
       log_lik[t] =  log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
      if (t < trial) {
        // 選択された選択肢のQ値の更新
        Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
        // 選択されなかった選択肢は更新しない
        Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
      }
    }
  }
}

```

q_learning_weak_info_single_WBIC.stan
```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}

parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
  real<lower=0.0> beta;            //逆温度
}

model {
  //学習率と逆温度の事前分布の指定はしていないので，parametersで指定した範囲の無情報事前分布が使われる
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  //学習率の事前分布にベータ分布，逆温度の事前分布にガンマ分布
  alpha ~ beta(2, 2); 
  beta ~ gamma(2, 0.5);
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += (1/log(trial))*log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}

generated quantities {
  vector[trial] log_lik;
  {
    matrix[trial, 2] Q;
    Q[1, 1] = 0;
    Q[1, 2] = 0;
    for ( t in 1:trial ) {
       log_lik[t] =  log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
      if (t < trial) {
        // 選択された選択肢のQ値の更新
        Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
        // 選択されなかった選択肢は更新しない
        Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
      }
    }
  }
}

```

## WBICによるモデル比較

推定してモデル比較してみましょう。

```{r}
# モデルのコンパイル
q_weak_info_single_WBIC <- cmdstan_model('rl/q_learning_weak_info_single_WBIC.stan')
q_non_info_single_WBIC <- cmdstan_model('rl/q_learning_non_info_single_WBIC.stan')

# データの読み込み
data_individual <- data_long %>% 
  filter(id == 2)

# MCMC
weak_info_WBIC <- q_weak_info_single_WBIC$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 10000,
        parallel_chains = 4)

non_info_WBIC <- q_non_info_single_WBIC$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 2000,
        parallel_chains = 4)

weak_info_WBIC_rstan <- rstan::read_stan_csv(weak_info_WBIC$output_files())
non_info_WBIC_rstan <- rstan::read_stan_csv(non_info_WBIC$output_files())
```

予想に反して，弱情報事前分布の方がWBICが低くなりました（低いほどよい）。弱と言うよりは結構強めな事前分布だったのでデータとの適合が悪かったのかもしれません。なお，sub02は，弱情報の方がWBICが小さくなりました。個人ごとには適切なモデルや事前分布は違う可能性があるので，階層モデルを作って評価が必要ですね。

```{r}
WBIC_weak <- -mean(rowSums(rstan::extract(weak_info_WBIC_rstan)$log_lik))
WBIC_non <- -mean(rowSums(rstan::extract(non_info_WBIC_rstan)$log_lik))
paste("弱情報事前分布のWBICは",WBIC_weak)
paste("無情報事前分布のWBICは",WBIC_non)
```


# 階層ベイズ

- これまで１名のデータをごとに推定をしてきたが，多くの心理学研究では，複数の参加者が研究に参加している。

- 個人ごとに推定する方法でも個人差は検討できるが，試行数が少ないとと推定は不安定になる。また，参加者間変動も考慮していない。

- 集団における各参加者のパラメタのばらつきもモデルに組み込んでベイズ推定する階層ベイズを用いると，個人差も検討することができ,推定も安定する。階層ベイズでは,個々の参加者から推定されるパラメタの分布も考慮できるので,個人のパラメタとグループのパラメタの傾向を同時制約的に推定できる。個人差も検討できるし、推定するパラメタも安定する。試行数を増やしにくい臨床研究で使いやすい！

- 階層ベイズモデルやモデル選択に関しては，「StanとRでベイズ統計モデリング」(松浦健太郎, 2016, 共立出版)，「行動データの計算論モデリング」（片平健太郎，2018，オーム社），「社会科学のためのベイズ統計モデリング」（浜田宏ら，2019，朝倉書店）などが詳しい。

# F モデル・シミュレーションと選択的影響テスト

- モデル・シミュレーションは，「強化学習モデルを使ったモデル・フィッティング１」で説明したので省略します。選択的影響テストとは，実験的操作を用いて，認知モデルの妥当性を検討する方法です（Heathcote et al., 2014）。選択的影響テストでは，認知モデルの特定のパラメータに影響すると考えられる実験的な操作をして，実際にパラメータに変化が生じるのかを検討します。この予想された変化をもって，パラメータとモデルの妥当性を示すことができます。例えば，反応時間に関する Drift-Diffusion Modelの妥当性検討では，色識別課題の条件を操作することで，操作に対応したパラメータの変化を確認していたりします(Voss, Rothermund, and Voss , 2004)。