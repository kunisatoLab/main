---
title: "強化学習モデル: ベイズ推定(2)"
---

## 認知モデリングの推奨実践法

Busemeyer & Diederich(2010), Heathcote (2014), Palminteri et al.(2017)を参考にまとめると，以下のような感じになります。

- A 認知課題と認知モデルを準備
- B 人工データ生成とパラメータリカバリーを確認（モデルや課題の修正）
- C データ収集と行動データを確認
- D パラメータ推定
- E 相対モデル比較
- F モデル・シミュレーションと選択的影響テスト

<a href="https://kunisatolab.github.io/main/how-to-cognitive-modeling-rl1.html" target="_blank">「強化学習モデル: 最尤推定」</a>と<a href="https://kunisatolab.github.io/main/how-to-cognitive-modeling-rl2.html" target="_blank">「強化学習モデル: ベイズ推定(1)」</a>では，ABCDとすすめてきました。今回は，今回は，EとFについて説明し（相対モデル比較に関連づけてモデル・リカバリーの説明もします），最後に階層モデルについて説明をします。

### 使用するRパッケージ

適宜必要なパッケージをインストールしてください。cmdstnrのインストールは，<a href="https://mc-stan.org/cmdstanr/" target="_blank">こちら</a>をご確認ください。

```{r message=FALSE, warning=FALSE}
rm(list = ls())
library(tidyverse)
library(cmdstanr)
library(rstan)
library(posterior)
library(bridgesampling)
```

改めて人工データを生成するために，実験状況の入ったsim_dataの準備とq_learning_sim関数とq_learning_ll関数の準備をします。

```{r, echo=TRUE, message=TRUE, warning=TRUE}
sim_data <- tibble(trial = 1:80,
                   prob_s1 = rep(c(0.2, 0.8), each = 40),
                   prob_s2 = rep(c(0.8, 0.2), each = 40),
                   reward_s1 = ifelse(runif(80) < prob_s1, 1, 0),
                   reward_s2 = ifelse(runif(80) < prob_s2, 1, 0))

q_learning_sim <- function(alpha, beta,data) {
  #変数の準備
  value_s1 <- 0          # s1の価値(初期値は0)
  value_s2 <- 0          # s2の価値(初期値は0)
  current_choice <- NULL     # ある時点の選択（1=s1，0=s2）
  choice_prob_s1 <- NULL  # s1の選択確率
  reward <- NULL                 # 報酬
  # Qlearningモデル
  for (i in 1:nrow(data)){
    # s1を選ぶ確率を計算し,一様分布から発生させた乱数が行動選択確率よりも小さい時に1（s1），大きい時に0（s2）
    choice_prob_s1[i] <- exp(beta*value_s1[i])/(exp(beta*value_s1[i])+exp(beta*value_s2[i]))
    current_choice[i] <- as.integer(runif(1,min=0,max=1) <= choice_prob_s1[i])
    #FBを報酬(r)として、価値の更新を行う。
    if (current_choice[i] == 1){
        reward[i] <- data$reward_s1[i]
        #予測誤差の計算
        prediction_error <-  reward[i] - value_s1[i]
        #予測誤差を使ってs1の価値を更新する
        value_s1[i+1] <- value_s1[i]+alpha*prediction_error
        #s2は更新なし
        value_s2[i+1] <- value_s2[i]
    }else{
        reward[i] <- data$reward_s2[i]
        #予測誤差の計算
        prediction_error <- reward[i] - value_s2[i]
        #予測誤差を使ってs2の価値を更新する
        value_s2[i+1] <- value_s2[i]+alpha*prediction_error
        #s1は更新なし
        value_s1[i+1] <- value_s1[i]
    }
  }
  result <- data.frame(trial = data$trial,
              value_s1 = value_s1[1:nrow(data)], 
              value_s2 = value_s2[1:nrow(data)], 
              prob_s1 = choice_prob_s1,
              choice = current_choice,
              reward = reward)
  return(result)
}
```


## E 相対モデル比較

sub01のデータの推定を考えると，弱情報事前分布の方が良さそうに思えますが，ここはモデル比較をしてみたいと思います。モデル比較する場合には，以下の２種類があります。今回は，予測をしたいという話ではないので，WBICを使用します。

- 予測の良さに基づくモデル選択法：AIC, 交差検証法，WAIC

- ベイズ的なモデル選択法(モデルと事前分布からみてどのくらいデータが意外か。この意外性が小さいほど良いモデルと考えられる)：BIC, WBIC, ベイズファクター,自由エネルギー


WBICの計算をする場合は，stanコードの対数尤度の計算の部分で対数尤度に(1/log(データ数)) を掛ける必要があります。さらに，generated quantitiesブロックで対数尤度を計算します。それぞれは，以下になります。

- q_learning_non_info_single_WBIC.stan

```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}
parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
  real<lower=0.0> beta;            //逆温度
}
model {
  //学習率と逆温度の事前分布の指定はしていないので，parametersで指定した範囲の無情報事前分布が使われる
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += (1/log(trial))*log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}
generated quantities {
  vector[trial] log_lik;
  {
    matrix[trial, 2] Q;
    Q[1, 1] = 0;
    Q[1, 2] = 0;
    for ( t in 1:trial ) {
       log_lik[t] =  log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
      if (t < trial) {
        // 選択された選択肢のQ値の更新
        Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
        // 選択されなかった選択肢は更新しない
        Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
      }
    }
  }
}
```

q_learning_weak_info_single_WBIC.stan

```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}
parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
  real<lower=0.0> beta;            //逆温度
}
model {
  //学習率と逆温度の事前分布の指定はしていないので，parametersで指定した範囲の無情報事前分布が使われる
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  //学習率の事前分布にベータ分布，逆温度の事前分布にガンマ分布
  alpha ~ beta(2, 2); 
  beta ~ gamma(2, 0.5);
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += (1/log(trial))*log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}
generated quantities {
  vector[trial] log_lik;
  {
    matrix[trial, 2] Q;
    Q[1, 1] = 0;
    Q[1, 2] = 0;
    for ( t in 1:trial ) {
       log_lik[t] =  log(exp(beta*Q[t,choice[t]])/(exp(beta*Q[t,choice[t]])+exp(beta*Q[t,3-choice[t]])));
      if (t < trial) {
        // 選択された選択肢のQ値の更新
        Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
        // 選択されなかった選択肢は更新しない
        Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
      }
    }
  }
}
```

### WBICによるモデル比較

推定してモデル比較してみましょう。まずは，モデルをコンパイルします。

```{r include=FALSE}
q_weak_info_single_WBIC <- cmdstan_model('rl/q_learning_weak_info_single_WBIC.stan')
q_non_info_single_WBIC <- cmdstan_model('rl/q_learning_non_info_single_WBIC.stan')
```


```
q_weak_info_single_WBIC <- cmdstan_model('q_learning_weak_info_single_WBIC.stan')
q_non_info_single_WBIC <- cmdstan_model('q_learning_non_info_single_WBIC.stan')
```

それでは，sub01で推定をしてみましょう！

```
data_individual <- data_long %>% 
  filter(id == 1)

weak_info_WBIC <- q_weak_info_single_WBIC$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 10000,
        parallel_chains = 4)

non_info_WBIC <- q_non_info_single_WBIC$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 2000,
        parallel_chains = 4)

weak_info_WBIC_rstan <- rstan::read_stan_csv(weak_info_WBIC$output_files())
non_info_WBIC_rstan <- rstan::read_stan_csv(non_info_WBIC$output_files())
```

```{r include=FALSE}
data_individual <- data_long %>% 
  filter(id == 1)

weak_info_WBIC <- q_weak_info_single_WBIC$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 3000,
        parallel_chains = 4)

non_info_WBIC <- q_non_info_single_WBIC$sample(
        data = list(trial = nrow(data_individual),
                   reward = data_individual$reward,
                   choice = data_individual$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 3000,
        parallel_chains = 4)

weak_info_WBIC_rstan <- rstan::read_stan_csv(weak_info_WBIC$output_files())
non_info_WBIC_rstan <- rstan::read_stan_csv(non_info_WBIC$output_files())
```

予想に反して，弱情報事前分布の方がWBICが低くなりました（低いほどよい）。弱と言うよりは結構強めな事前分布だったのでデータとの適合が悪かったのかもしれません。なお，sub02は，弱情報の方がWBICが小さくなりました。個人ごとには適切なモデルや事前分布は違う可能性があるので，階層モデルを作って評価が必要ですね。

```{r}
WBIC_weak <- -mean(rowSums(rstan::extract(weak_info_WBIC_rstan)$log_lik))
WBIC_non <- -mean(rowSums(rstan::extract(non_info_WBIC_rstan)$log_lik))
paste("弱情報事前分布のWBICは",WBIC_weak)
paste("無情報事前分布のWBICは",WBIC_non)
```


## モデルリカバリー

これまでパラメータリカバリーをパラメータ推定のチェックで使ってきた。しかし，実際は複数の生成モデルが用いられることがある。パラメータ推定後の相対モデル比較において，データ生成過程で用いられた生成モデルがちゃんと選ばれるか確認するモデルリカバリーという考えもある。例えば，特定の生成モデルAから生成されたシミュレーションデータに対して，複数の生成モデルA ,B,Cを用いてパラメータ推定をした場合に，モデル比較においてBやCよりもAが選ばれる必要がある。モデルリカバリーは，全ての生成モデルから異なる値のパラメータを用いてシミュレーションデータを生成し，それらに対して，全ての生成モデルを用いてパラメータ推定を行うことを全ての組み合わせで行う。モデルリカバリーによって，検討しているモデルが研究目的から妥当か，追加すべきモデルはないか，パラメータ推定で識別不可能なモデルはないかを検討することができる。


### モデルリカバリーで用いるモデル

今回は２つの生成モデルを考える。

- 上記で検討した事前分布をおいた強化学習モデル
- 上記からβを除外した強化学習モデル（β=1に仮定したモデルともいえる）


#### αとβを仮定したモデルから生成されたデータのモデルリカバリー

まずは，これまでと同様なαもβも存在する強化学習モデルでデータを生成します。それに対して，それと一致したモデル（上記のq_learning_weak_info_single_WBIC.stan）と一致しないモデル（以下のq_learning_weak_info_no_beta_WBIC.stan）を使って推定することで，モデルリカバリーをしてみます。

q_learning_weak_info_no_beta_WBIC.stanは，以下のようなコードになります。簡単にいうと，q_learning_weak_info_single_WBIC.stanからbetaを抜いただけです。

```
data {
  int<lower=1> trial;
  int<lower=1,upper=2> choice[trial]; // 1 or 2
  int<lower=0,upper=1> reward[trial]; // 0 or 1
}

parameters {
  real<lower=0.0,upper=1.0> alpha; //学習率
}

model {
  //学習率と逆温度の事前分布の指定はしていないので，parametersで指定した範囲の無情報事前分布が使われる
  matrix[trial,2] Q;
  Q[1, 1] = 0;
  Q[1, 2] = 0;
  
  //学習率の事前分布にベータ分布，逆温度の事前分布にガンマ分布
  alpha ~ beta(2, 2);
  
  for ( t in 1:trial) {
    // 対数尤度を足す
    target += (1/log(trial))*log(exp(Q[t,choice[t]])/(exp(Q[t,choice[t]])+exp(Q[t,3-choice[t]])));
    
    if (t < trial) {
      // 選択された選択肢のQ値の更新
      Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
      // 選択されなかった選択肢は更新しない
      Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
    }
  }
}

generated quantities {
  vector[trial] log_lik;
  {
    matrix[trial, 2] Q;
    Q[1, 1] = 0;
    Q[1, 2] = 0;
    for ( t in 1:trial ) {
       log_lik[t] =  log(exp(Q[t,choice[t]])/(exp(Q[t,choice[t]])+exp(Q[t,3-choice[t]])));
      if (t < trial) {
        // 選択された選択肢のQ値の更新
        Q[t+1,choice[t]] = Q[t, choice[t]] + alpha * (reward[t] - Q[t, choice[t]]);
        // 選択されなかった選択肢は更新しない
        Q[t+1, 3- choice[t]] = Q[t, 3- choice[t]];
      }
    }
  }
}
```

まずは，q_learning_weak_info_no_beta_WBIC.stanをコンパイルします。

```{r include=FALSE}
q_weak_info_no_beta_WBIC <- cmdstan_model('rl/q_learning_weak_info_no_beta_WBIC.stan')
```


```
q_weak_info_no_beta_WBIC <- cmdstan_model('rl/q_learning_weak_info_no_beta_WBIC.stan')
```

さて，以下のコードを走らせて，αとβを仮定したモデルで生成されたデータをαとβを仮定するモデルとαだけを仮定するモデルでフィッティングした時に，モデルが選択される確率を計算します。





やっぱランダムサンプリングにする。





```{r include=FALSE}
better_with_beta <- 0
beta_set <- 0
for (i in 1:10) {
  alpha_set <- 0
  beta_set = beta_set + 0.5
  for (j in 1:10) {
    alpha_set = alpha_set + 0.1
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    print(paste("進捗状況：",(i-1)*10 + j,"/100"))
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      with_beta_WBIC <- q_weak_info_single_WBIC$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 3000,
        parallel_chains = 4)

      no_beta_WBIC <- q_weak_info_no_beta_WBIC$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 3000,
        parallel_chains = 4)
      
      with_beta_WBIC_sample <- rstan::read_stan_csv(with_beta_WBIC$output_files())
      no_beta_WBIC_sample <- rstan::read_stan_csv(no_beta_WBIC$output_files())
      with_beta_WBIC <- -mean(rowSums(rstan::extract(weak_info_WBIC_rstan)$log_lik))
      no_beta_WBIC <- -mean(rowSums(rstan::extract(no_beta_WBIC_sample)$log_lik))
      better_with_beta <- better_with_beta + ifelse(with_beta_WBIC < no_beta_WBIC, 1, 0)
    },error = function(e) {message(e)})
  }
}

fit_both_sim_both <- better_with_beta/100
fit_alpha_sim_both <- (100-better_with_beta)/100

```

#### αのみを仮定したモデルから生成されたデータのモデルリカバリー

次は，αのみを仮定したモデル（βは仮定しない，つまり，βを１に固定するモデル）で生成されたデータをαとβを仮定するモデルとαだけを仮定するモデルでフィッティングした時に，モデルが選択される確率を計算します。

```{r include=FALSE}
better_with_beta <- 0
beta_set <- 1
alpha_set <- 0
for (i in 1:100) {
    alpha_set = alpha_set + 0.01
    #データ生成
    data_2 <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
    print(paste("進捗状況：",(i-1)*10 + j,"/100"))
    #パラメータ推定(推定がミスった時用にtryCatch関数を準備)
    tryCatch({
      with_beta_WBIC <- q_weak_info_single_WBIC$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 3000,
        parallel_chains = 4)

      no_beta_WBIC <- q_weak_info_no_beta_WBIC$sample(
        data = list(trial = nrow(data_2),
                   reward = data_2$reward,
                   choice = data_2$choice + 1),
        seed = 123,
        chains = 4,
        iter_warmup = 500,
        iter_sampling = 3000,
        parallel_chains = 4)
      
      with_beta_WBIC_sample <- rstan::read_stan_csv(with_beta_WBIC$output_files())
      with_beta_WBIC <- -mean(rowSums(rstan::extract(weak_info_WBIC_rstan)$log_lik))
      no_beta_WBIC_sample <- rstan::read_stan_csv(no_beta_WBIC$output_files())
      no_beta_WBIC <- -mean(rowSums(rstan::extract(no_beta_WBIC_sample)$log_lik))
      better_with_beta <- better_with_beta + ifelse(with_beta_WBIC < no_beta_WBIC, 1, 0)
    },error = function(e) {message(e)})
}

fit_both_sim_alpha <- better_with_beta/100
fit_alpha_sim_alpha <- (100-better_with_beta)/100

```


整理すると以下のようになります。

```{r}
simulated_model <- c()
fit_model <- c()
prob <- c(fit_both_sim_both, fit_alpha_sim_both, fit_both_sim_alpha, fit_alpha_sim_alpha)

model_recovery <- tibble(simulated_model, fit_model, prob)

ggplot(model_recovery, aes(x = fit_model, y = simulated_model, fill = prob)) +
  geom_tile() +
  geom_text(aes(label = round(mean_c1, digits = 1)), color = "white")
```


## 階層ベイズ

- これまで１名のデータをごとに推定をしてきたが，多くの心理学研究では，複数の参加者が研究に参加している。

- 個人ごとに推定する方法でも個人差は検討できるが，試行数が少ないとと推定は不安定になる。また，参加者間変動も考慮していない。

- 集団における各参加者のパラメタのばらつきもモデルに組み込んでベイズ推定する階層ベイズを用いると，個人差も検討することができ,推定も安定する。階層ベイズでは,個々の参加者から推定されるパラメタの分布も考慮できるので,個人のパラメタとグループのパラメタの傾向を同時制約的に推定できる。個人差も検討できるし、推定するパラメタも安定する。試行数を増やしにくい臨床研究で使いやすい！


上記の２つのモデルで作成して，比較してみる。









- 階層ベイズモデルやモデル選択に関しては，「StanとRでベイズ統計モデリング」(松浦健太郎, 2016, 共立出版)，「行動データの計算論モデリング」（片平健太郎，2018，オーム社），「社会科学のためのベイズ統計モデリング」（浜田宏ら，2019，朝倉書店）などが詳しい。

## F モデル・シミュレーションと選択的影響テスト

モデル・シミュレーションでは，<a href="https://kunisatolab.github.io/main/how-to-cognitive-modeling-rl1.html" target="_blank">「強化学習モデル: 最尤推定」</a>で行ったような人工データ生成を最もフィットの良かった推定値を用いて行います。複数のモデルの中で最もデータ適合が良いモデルであったとしても，それは相対的に良いモデルになります。私達が想定したモデルはどれも悪いモデルだった場合は，相対的には良いモデルを選ぶことはできても，それが絶対的には悪いモデルある可能性があります。そこで，情報量規準などで最も良いとされたモデルの推定値を用いて，人工データ生成を行って，その生成されたデータが現実のデータに近いものかを評価します。モデルシミュレーションについては，Palminteri, Wyart, & Koechlin(2017)が詳しいです。


選択的影響テストとは，実験的操作を用いて，認知モデルの妥当性を検討する方法です（Heathcote et al., 2015）。選択的影響テストでは，認知モデルの特定のパラメータに影響すると考えられる実験的な操作をして，実際にパラメータに変化が生じるのかを検討します。この予想された変化をもって，パラメータとモデルの妥当性を示すことができます。例えば，反応時間に関する Drift-Diffusion Modelの妥当性検討では，色識別課題の条件を操作することで，操作に対応したパラメータの変化を確認していたりします(Voss, Rothermund, and Voss , 2004)。


## 引用・参考文献

- Busemeyer, J. R., & Diederich, A. (2010). Cognitive Modeling. SAGE.
- Heathcote, A., Brown, S. D., & Wagenmakers, E.-J. (2015). An Introduction to Good Practices in Cognitive Modeling. In B. U. Forstmann & E.-J. Wagenmakers (Eds.), An Introduction to Model-Based Cognitive Neuroscience (pp. 25–48). Springer New York.
- Palminteri, S., Wyart, V., & Koechlin, E. (2017). The Importance of Falsification in Computational Cognitive Modeling. Trends in Cognitive Sciences, 21(6), 425–433.
- Voss, A., Rothermund, K., & Voss, J. (2004). Interpreting the parameters of the diffusion model: an empirical validation. Memory & Cognition, 32(7), 1206–1220.
- 松浦健太郎(2016).StanとRでベイズ統計モデリング 共立出版
- 片平健太郎(2018). 行動データの計算論モデリング オーム社
- 浜田宏ら(2019). 社会科学のためのベイズ統計モデリング 朝倉書店
