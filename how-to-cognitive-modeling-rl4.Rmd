---
title: "強化学習モデル: ベイズ推定(3)"
---

## 階層ベイズ

- これまで１名のデータをごとに推定をしてきたが，多くの心理学研究では，複数の参加者が研究に参加している。

- 個人ごとに推定する方法でも個人差は検討できるが，試行数が少ないとと推定は不安定になる。また，参加者間変動も考慮していない。

- 集団における各参加者のパラメタのばらつきもモデルに組み込んでベイズ推定する階層ベイズを用いると，個人差も検討することができ,推定も安定する。階層ベイズでは,個々の参加者から推定されるパラメタの分布も考慮できるので,個人のパラメタとグループのパラメタの傾向を同時制約的に推定できる。個人差も検討できるし、推定するパラメタも安定する。試行数を増やしにくい臨床研究で使いやすい！

### 使用するRパッケージ

```{r}
library(tidyverse)
library(cmdstanr)
library(rstan)
library(posterior)
library(bridgesampling)
```


### シミュレーションデータの用意

おなじみのQ learning関数を使います。

```{r}
sim_data <- tibble(trial = 1:80,
                   prob_s1 = rep(c(0.2, 0.8), each = 40),
                   prob_s2 = rep(c(0.8, 0.2), each = 40),
                   reward_s1 = ifelse(runif(80) < prob_s1, 1, 0),
                   reward_s2 = ifelse(runif(80) < prob_s2, 1, 0))

q_learning_sim <- function(alpha, beta,data) {
  #変数の準備
  value_s1 <- 0          # s1の価値(初期値は0)
  value_s2 <- 0          # s2の価値(初期値は0)
  current_choice <- NULL     # ある時点の選択（1=s1，0=s2）
  choice_prob_s1 <- NULL  # s1の選択確率
  reward <- NULL                 # 報酬
  # Qlearningモデル
  for (i in 1:nrow(data)){
    # s1を選ぶ確率を計算し,一様分布から発生させた乱数が行動選択確率よりも小さい時に1（s1），大きい時に0（s2）
    choice_prob_s1[i] <- exp(beta*value_s1[i])/(exp(beta*value_s1[i])+exp(beta*value_s2[i]))
    current_choice[i] <- as.integer(runif(1,min=0,max=1) <= choice_prob_s1[i])
    #FBを報酬(r)として、価値の更新を行う。
    if (current_choice[i] == 1){
        reward[i] <- data$reward_s1[i]
        #予測誤差の計算
        prediction_error <-  reward[i] - value_s1[i]
        #予測誤差を使ってs1の価値を更新する
        value_s1[i+1] <- value_s1[i]+alpha*prediction_error
        #s2は更新なし
        value_s2[i+1] <- value_s2[i]
    }else{
        reward[i] <- data$reward_s2[i]
        #予測誤差の計算
        prediction_error <- reward[i] - value_s2[i]
        #予測誤差を使ってs2の価値を更新する
        value_s2[i+1] <- value_s2[i]+alpha*prediction_error
        #s1は更新なし
        value_s1[i+1] <- value_s1[i]
    }
  }
  result <- data.frame(trial = data$trial,
              value_s1 = value_s1[1:nrow(data)], 
              value_s2 = value_s2[1:nrow(data)], 
              prob_s1 = choice_prob_s1,
              choice = current_choice,
              reward = reward)
  return(result)
}

```


シミュレーションデータを100個生成します。とりあえず，αをベータ分布(Beta(2,2))からβをガンマ分布(Gamma(2,0.5))から生成するとします。

```{r}
sim_data_long <- NULL
true_alpha <- NULL
true_beta <- NULL

for (i in 1:100) {
  alpha_set <- rbeta(1, 2, 2)
  beta_set <- rgamma(1, 2, 0.5)
  true_alpha[i] <- alpha_set
  true_beta[i] <- beta_set
  data_tmp <- q_learning_sim(alpha = alpha_set, beta = beta_set, sim_data)
  data_tmp$id <- rep(i,80)
  sim_data_long <- rbind(sim_data_long,data_tmp)
}

true_parameter <- tibble(id = 1:100,true_alpha, true_beta)
```





```
コードが確定したら挿入する
```



上記の２つのモデルで作成して，比較してみる。

- 階層ベイズモデルやモデル選択に関しては，「StanとRでベイズ統計モデリング」(松浦健太郎, 2016, 共立出版)，「行動データの計算論モデリング」（片平健太郎，2018，オーム社），「社会科学のためのベイズ統計モデリング」（浜田宏ら，2019，朝倉書店）などが詳しい。


## 引用・参考文献

- Schaaf, J. V., Jepma, M., Visser, I., & Huizenga, H. M. (2019). A hierarchical Bayesian approach to assess learning and guessing strategies in reinforcement learning. Journal of Mathematical Psychology, 93, 102276.
- Steingroever, H., Wetzels, R., & Wagenmakers, E.-J. (2014). Absolute performance of reinforcement-learning models for the Iowa Gambling Task. Decision (Washington, D.C.), 1(3), 161–183.
- 松浦健太郎(2016).StanとRでベイズ統計モデリング 共立出版
- 片平健太郎(2018). 行動データの計算論モデリング オーム社
- 浜田宏ら(2019). 社会科学のためのベイズ統計モデリング 朝倉書店